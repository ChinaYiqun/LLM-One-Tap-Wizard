{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install gradio\n",
        "!pip install --upgrade pip\n",
        "!pip install uv\n",
        "!uv pip install \"sglang[all]>=0.4.6.post2\"\n",
        "!uv pip install vllm # If you are using uv.\n",
        "!uv pip install pexpect\n",
        "!uv pip install autoawq"
      ],
      "metadata": {
        "id": "ARXw70RJh1bn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2755bab2-58bc-463c-d83d-1de59bc905e4"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "####                                                                       6.5%^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install colab-xterm\n",
        "# 加载colab-xterm扩展\n",
        "%load_ext colabxterm\n",
        "# !ollama serve &\n",
        "# !curl https://ollama.com/install.sh | sh\n",
        "# !ollama run hf.co/jnjj/Qwen3-0.6B-Q2_K-GGUF:Q2_K"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vPqgdEV5pKso",
        "outputId": "27d4447d-b4d3-465f-c471-3ada020175a4"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting colab-xterm\n",
            "  Downloading colab_xterm-0.2.0-py3-none-any.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: ptyprocess~=0.7.0 in /usr/local/lib/python3.11/dist-packages (from colab-xterm) (0.7.0)\n",
            "Requirement already satisfied: tornado>5.1 in /usr/local/lib/python3.11/dist-packages (from colab-xterm) (6.4.2)\n",
            "Downloading colab_xterm-0.2.0-py3-none-any.whl (115 kB)\n",
            "Installing collected packages: colab-xterm\n",
            "Successfully installed colab-xterm-0.2.0\n",
            "Error: could not connect to ollama app, is it running?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "import subprocess\n",
        "import shlex\n",
        "import signal\n",
        "import requests\n",
        "\n",
        "# 用于保存当前运行的子进程\n",
        "current_process = None\n",
        "\n",
        "def run_command(command):\n",
        "    \"\"\"执行命令并实时返回输出\"\"\"\n",
        "    global current_process\n",
        "    try:\n",
        "        args = shlex.split(command)\n",
        "        process = subprocess.Popen(\n",
        "            args,\n",
        "            stdout=subprocess.PIPE,\n",
        "            stderr=subprocess.STDOUT,\n",
        "            text=True,\n",
        "            bufsize=1,\n",
        "            universal_newlines=True\n",
        "        )\n",
        "        current_process = process  # 保存当前进程对象\n",
        "\n",
        "        while True:\n",
        "            output = process.stdout.readline()\n",
        "            if output == '' and process.poll() is not None:\n",
        "                break\n",
        "            if output:\n",
        "                yield output.strip()\n",
        "\n",
        "        remaining = process.stdout.read()\n",
        "        if remaining:\n",
        "            yield remaining.strip()\n",
        "\n",
        "    except Exception as e:\n",
        "        yield f\"错误: {str(e)}\"\n",
        "    finally:\n",
        "        current_process = None  # 清除当前进程引用\n",
        "\n",
        "def stop_process():\n",
        "    \"\"\"终止当前运行的子进程\"\"\"\n",
        "    global current_process\n",
        "    if current_process is not None:\n",
        "        # 先尝试优雅终止\n",
        "        current_process.send_signal(signal.SIGTERM)\n",
        "        try:\n",
        "            current_process.wait(timeout=0.5)\n",
        "        except subprocess.TimeoutExpired:\n",
        "            # 若仍未关闭，强制杀死\n",
        "            current_process.kill()\n",
        "        return \"进程已终止\"\n",
        "    else:\n",
        "        return \"当前没有运行的进程\"\n",
        "\n",
        "\n",
        "#生成大模型部署命令\n",
        "def generate_command(model_path, deploy_type):\n",
        "    \"\"\"根据模型路径和部署类型生成完整命令\"\"\"\n",
        "    common_params = \"--host 0.0.0.0 --port 30000\"\n",
        "    if deploy_type == \"sglang\":\n",
        "        return f\"python3 -m sglang.launch_server --model-path {model_path} {common_params}\"\n",
        "    elif deploy_type == \"vllm serve\":\n",
        "        return f\"vllm serve {model_path} {common_params}\"\n",
        "    else:\n",
        "        return \"错误：未知的部署类型\"\n",
        "#生成模型下载命令\n",
        "def generate_download_command(git_path, savepath):\n",
        "    \"\"\"\n",
        "    生成模型下载命令\n",
        "    :param git_path: 模型的 Git 仓库地址，例如 \"https://huggingface.co/Qwen/Qwen2-7B \"\n",
        "    :param savepath: 本地保存路径，例如 \"./Qwen2-7B\"\n",
        "    :return: 下载命令字符串\n",
        "    \"\"\"\n",
        "    return f\"git clone {git_path} {savepath}\"\n",
        "\n",
        "def generate_quantize_command(model_path, savepath):\n",
        "    from awq import AutoAWQForCausalLM\n",
        "    from transformers import AutoTokenizer\n",
        "\n",
        "    model_path = model_path\n",
        "    quant_path = savepath\n",
        "    quant_config = { \"zero_point\": True, \"q_group_size\": 128, \"w_bit\": 4, \"version\": \"GEMM\" }\n",
        "\n",
        "    # Load model\n",
        "    model = AutoAWQForCausalLM.from_pretrained(\n",
        "        model_path, **{\"low_cpu_mem_usage\": True, \"use_cache\": False}\n",
        "    )\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)\n",
        "\n",
        "    # Quantize\n",
        "    model.quantize(tokenizer, quant_config=quant_config)\n",
        "\n",
        "    # Save quantized model\n",
        "    model.save_quantized(quant_path)\n",
        "    tokenizer.save_pretrained(quant_path)\n",
        "\n",
        "    return(f'Model is quantized and saved at \"{quant_path}\"')\n",
        "\n",
        "# 检查服务是否运行\n",
        "def is_server_running():\n",
        "    try:\n",
        "        res = requests.get(\"http://localhost:30000/health\")\n",
        "        return res.status_code == 200\n",
        "    except:\n",
        "        return False\n",
        "\n",
        "# 获取当前服务状态\n",
        "def get_status():\n",
        "    return \"🟢 运行中\" if is_server_running() else \"🔴 未运行\"\n",
        "\n",
        "def model_predict(message, chat_history):\n",
        "    try:\n",
        "        response = requests.post(\"http://localhost:30000/generate\", json={\n",
        "            \"text\": message,\n",
        "            \"sampling_params\": {\n",
        "                \"temperature\": 0.7,\n",
        "                \"max_new_tokens\": 256\n",
        "            }\n",
        "        })\n",
        "        response.raise_for_status()\n",
        "        data = response.json()\n",
        "\n",
        "        chat_history.append({\"role\": \"user\", \"content\": message})\n",
        "        chat_history.append({\"role\": \"assistant\", \"content\": data[\"text\"]})\n",
        "\n",
        "        return \"\", chat_history\n",
        "\n",
        "    except requests.exceptions.ConnectionError:\n",
        "        return \"\", [{\"role\": \"system\", \"content\": \"❌ 无法连接到服务，请先启动模型服务器\"}]\n",
        "    except Exception as e:\n",
        "        return \"\", [{\"role\": \"system\", \"content\": f\"❌ 错误: {str(e)}\"}]"
      ],
      "metadata": {
        "id": "zhZjlHr3hwQ6"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 创建 Gradio 界面\n",
        "with gr.Blocks(title=\"极简大模型部署脚本\") as demo:\n",
        "    with gr.Tab(\"🏗︎ 部署操作\"):\n",
        "        gr.Markdown(\"# 📋 命令行执行器\\n在下方输入命令，按回车执行\")\n",
        "\n",
        "        model_path_input = gr.Textbox(\n",
        "            label=\"模型路径\",\n",
        "            placeholder=\"例如：Qwen/Qwen3-0.6B\",\n",
        "            value=\"Qwen/Qwen3-0.6B\"\n",
        "        )\n",
        "\n",
        "        with gr.Row():\n",
        "            deploy_type = gr.Radio(\n",
        "                choices=[\"sglang\", \"vllm serve\"],\n",
        "                label=\"部署类型\",\n",
        "                value=\"sglang\",\n",
        "                scale=3\n",
        "            )\n",
        "            deploy_button = gr.Button(\"确认部署\", scale=1)\n",
        "\n",
        "        command_input = gr.Textbox(\n",
        "            label=\"输入命令\",\n",
        "            placeholder=\"例如：ping 8.8.8.8 或 ls -l\",\n",
        "            value=\"echo Hello World\"\n",
        "        )\n",
        "\n",
        "        output_box = gr.Textbox(\n",
        "            label=\"日志输出\",\n",
        "            lines=20,\n",
        "            max_lines=50,\n",
        "            interactive=False\n",
        "        )\n",
        "\n",
        "        with gr.Row():\n",
        "            stop_button = gr.Button(\"终止进程\", variant=\"stop\")\n",
        "            # 命令生成\n",
        "        deploy_button.click(\n",
        "            fn=generate_command,\n",
        "            inputs=[model_path_input, deploy_type],\n",
        "            outputs=command_input\n",
        "        )\n",
        "\n",
        "        # 命令执行\n",
        "        command_input.submit(fn=run_command, inputs=command_input, outputs=output_box, scroll_to_output=True)\n",
        "\n",
        "        # 终止命令\n",
        "        stop_button.click(fn=stop_process, inputs=[], outputs=output_box)\n",
        "\n",
        "\n",
        "    with gr.Tab(\"服务监控与聊天测试\"):\n",
        "        gr.Markdown(\"# 🤖 大模型服务监控\")\n",
        "\n",
        "        status_box = gr.Textbox(\n",
        "            label=\"📡 服务状态\",\n",
        "            value=get_status,\n",
        "            every=3,\n",
        "            interactive=False\n",
        "        )\n",
        "\n",
        "        with gr.Row():\n",
        "            chatbot = gr.Chatbot(\n",
        "                label=\"对话历史\",\n",
        "                bubble_full_width=False,\n",
        "                type=\"messages\"\n",
        "            )\n",
        "            msg = gr.Textbox(\n",
        "                label=\"输入问题\",\n",
        "                lines=5,\n",
        "                placeholder=\"输入你的问题...\",\n",
        "            )\n",
        "\n",
        "        with gr.Row():\n",
        "            clear = gr.ClearButton([msg, chatbot])\n",
        "            send_button = gr.Button(\"发送\", variant=\"primary\")\n",
        "\n",
        "        msg.submit(fn=model_predict, inputs=[msg, chatbot], outputs=[msg, chatbot], queue=True)\n",
        "        send_button.click(fn=model_predict, inputs=[msg, chatbot], outputs=[msg, chatbot])\n",
        "\n",
        "    with gr.Tab(\"模型下载\"):\n",
        "        gr.Markdown(\"# ⬇️ 模型下载\")\n",
        "        with gr.Row():\n",
        "            model_path_input = gr.Textbox(\n",
        "                label=\"模型路径\",\n",
        "                placeholder=\"https://huggingface.co/Qwen/Qwen3-0.6B\",\n",
        "                value=\"https://huggingface.co/Qwen/Qwen3-0.6B\"\n",
        "            )\n",
        "            savepath_input = gr.Textbox(\n",
        "                label=\"保存路径\",\n",
        "                placeholder=\"./models\",\n",
        "                value=\"./models\"\n",
        "            )\n",
        "            dowload_button = gr.Button(\"确认下载\", scale=1)\n",
        "        command_input = gr.Textbox(\n",
        "            label=\"输入命令\",\n",
        "            placeholder=\"例如：ping 8.8.8.8 或 ls -l\",\n",
        "            value=\"echo Hello World\"\n",
        "        )\n",
        "\n",
        "        output_box = gr.Textbox(\n",
        "            label=\"日志输出\",\n",
        "            lines=20,\n",
        "            max_lines=50,\n",
        "            interactive=False\n",
        "        )\n",
        "\n",
        "\n",
        "        with gr.Row():\n",
        "            stop_button = gr.Button(\"终止进程\", variant=\"stop\")\n",
        "            # 命令生成\n",
        "        dowload_button.click(\n",
        "            fn=generate_download_command,\n",
        "            inputs=[model_path_input, savepath_input],\n",
        "            outputs=command_input\n",
        "        )\n",
        "        # 命令执行\n",
        "        command_input.submit(fn=run_command, inputs=command_input, outputs=output_box, scroll_to_output=True)\n",
        "\n",
        "        # 终止命令\n",
        "        stop_button.click(fn=stop_process, inputs=[], outputs=output_box)\n",
        "    with gr.Tab(\"模型量化\"):\n",
        "        gr.Markdown(\"# 🔹模型量化\")\n",
        "        with gr.Row():\n",
        "            model_path_input = gr.Textbox(\n",
        "                label=\"模型路径\",\n",
        "                placeholder=\"https://huggingface.co/Qwen/Qwen3-0.6B\",\n",
        "                value=\"https://huggingface.co/Qwen/Qwen3-0.6B\"\n",
        "            )\n",
        "            savepath_input = gr.Textbox(\n",
        "                label=\"保存路径\",\n",
        "                placeholder=\"./models\",\n",
        "                value=\"./models\"\n",
        "            )\n",
        "            dowload_button = gr.Button(\"确认量化\", scale=1)\n",
        "        command_input = gr.Textbox(\n",
        "            label=\"量化结果\",\n",
        "        )\n",
        "\n",
        "        output_box = gr.Textbox(\n",
        "            label=\"日志输出\",\n",
        "            lines=20,\n",
        "            max_lines=50,\n",
        "            interactive=False\n",
        "        )\n",
        "\n",
        "\n",
        "        with gr.Row():\n",
        "            stop_button = gr.Button(\"终止进程\", variant=\"stop\")\n",
        "            # 命令生成\n",
        "        dowload_button.click(\n",
        "            fn=generate_quantize_command,\n",
        "            inputs=[model_path_input, savepath_input],\n",
        "            outputs=command_input\n",
        "        )\n",
        "        # 命令执行\n",
        "        command_input.submit(fn=run_command, inputs=command_input, outputs=output_box, scroll_to_output=True)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    demo.launch(share=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 625
        },
        "id": "phtiZExgIakx",
        "outputId": "8a844d65-0009-4631-d67b-f3c006981369"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-9-c6be29598f15>:61: DeprecationWarning: The 'bubble_full_width' parameter is deprecated and will be removed in a future version. This parameter no longer has any effect.\n",
            "  chatbot = gr.Chatbot(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://b4e45fe17eaa7a8e67.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://b4e45fe17eaa7a8e67.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}