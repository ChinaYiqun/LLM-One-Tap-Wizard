{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install gradio\n",
        "!pip install --upgrade pip\n",
        "!pip install uv\n",
        "!uv pip install \"sglang[all]>=0.4.6.post2\"\n",
        "!uv pip install vllm # If you are using uv.\n",
        "!uv pip install pexpect\n",
        "!uv pip install autoawq"
      ],
      "metadata": {
        "id": "ARXw70RJh1bn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2755bab2-58bc-463c-d83d-1de59bc905e4"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "####                                                                       6.5%^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install colab-xterm\n",
        "# åŠ è½½colab-xtermæ‰©å±•\n",
        "%load_ext colabxterm\n",
        "# !ollama serve &\n",
        "# !curl https://ollama.com/install.sh | sh\n",
        "# !ollama run hf.co/jnjj/Qwen3-0.6B-Q2_K-GGUF:Q2_K"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vPqgdEV5pKso",
        "outputId": "27d4447d-b4d3-465f-c471-3ada020175a4"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting colab-xterm\n",
            "  Downloading colab_xterm-0.2.0-py3-none-any.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: ptyprocess~=0.7.0 in /usr/local/lib/python3.11/dist-packages (from colab-xterm) (0.7.0)\n",
            "Requirement already satisfied: tornado>5.1 in /usr/local/lib/python3.11/dist-packages (from colab-xterm) (6.4.2)\n",
            "Downloading colab_xterm-0.2.0-py3-none-any.whl (115 kB)\n",
            "Installing collected packages: colab-xterm\n",
            "Successfully installed colab-xterm-0.2.0\n",
            "Error: could not connect to ollama app, is it running?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "import subprocess\n",
        "import shlex\n",
        "import signal\n",
        "import requests\n",
        "\n",
        "# ç”¨äºä¿å­˜å½“å‰è¿è¡Œçš„å­è¿›ç¨‹\n",
        "current_process = None\n",
        "\n",
        "def run_command(command):\n",
        "    \"\"\"æ‰§è¡Œå‘½ä»¤å¹¶å®æ—¶è¿”å›è¾“å‡º\"\"\"\n",
        "    global current_process\n",
        "    try:\n",
        "        args = shlex.split(command)\n",
        "        process = subprocess.Popen(\n",
        "            args,\n",
        "            stdout=subprocess.PIPE,\n",
        "            stderr=subprocess.STDOUT,\n",
        "            text=True,\n",
        "            bufsize=1,\n",
        "            universal_newlines=True\n",
        "        )\n",
        "        current_process = process  # ä¿å­˜å½“å‰è¿›ç¨‹å¯¹è±¡\n",
        "\n",
        "        while True:\n",
        "            output = process.stdout.readline()\n",
        "            if output == '' and process.poll() is not None:\n",
        "                break\n",
        "            if output:\n",
        "                yield output.strip()\n",
        "\n",
        "        remaining = process.stdout.read()\n",
        "        if remaining:\n",
        "            yield remaining.strip()\n",
        "\n",
        "    except Exception as e:\n",
        "        yield f\"é”™è¯¯: {str(e)}\"\n",
        "    finally:\n",
        "        current_process = None  # æ¸…é™¤å½“å‰è¿›ç¨‹å¼•ç”¨\n",
        "\n",
        "def stop_process():\n",
        "    \"\"\"ç»ˆæ­¢å½“å‰è¿è¡Œçš„å­è¿›ç¨‹\"\"\"\n",
        "    global current_process\n",
        "    if current_process is not None:\n",
        "        # å…ˆå°è¯•ä¼˜é›…ç»ˆæ­¢\n",
        "        current_process.send_signal(signal.SIGTERM)\n",
        "        try:\n",
        "            current_process.wait(timeout=0.5)\n",
        "        except subprocess.TimeoutExpired:\n",
        "            # è‹¥ä»æœªå…³é—­ï¼Œå¼ºåˆ¶æ€æ­»\n",
        "            current_process.kill()\n",
        "        return \"è¿›ç¨‹å·²ç»ˆæ­¢\"\n",
        "    else:\n",
        "        return \"å½“å‰æ²¡æœ‰è¿è¡Œçš„è¿›ç¨‹\"\n",
        "\n",
        "\n",
        "#ç”Ÿæˆå¤§æ¨¡å‹éƒ¨ç½²å‘½ä»¤\n",
        "def generate_command(model_path, deploy_type):\n",
        "    \"\"\"æ ¹æ®æ¨¡å‹è·¯å¾„å’Œéƒ¨ç½²ç±»å‹ç”Ÿæˆå®Œæ•´å‘½ä»¤\"\"\"\n",
        "    common_params = \"--host 0.0.0.0 --port 30000\"\n",
        "    if deploy_type == \"sglang\":\n",
        "        return f\"python3 -m sglang.launch_server --model-path {model_path} {common_params}\"\n",
        "    elif deploy_type == \"vllm serve\":\n",
        "        return f\"vllm serve {model_path} {common_params}\"\n",
        "    else:\n",
        "        return \"é”™è¯¯ï¼šæœªçŸ¥çš„éƒ¨ç½²ç±»å‹\"\n",
        "#ç”Ÿæˆæ¨¡å‹ä¸‹è½½å‘½ä»¤\n",
        "def generate_download_command(git_path, savepath):\n",
        "    \"\"\"\n",
        "    ç”Ÿæˆæ¨¡å‹ä¸‹è½½å‘½ä»¤\n",
        "    :param git_path: æ¨¡å‹çš„ Git ä»“åº“åœ°å€ï¼Œä¾‹å¦‚ \"https://huggingface.co/Qwen/Qwen2-7B \"\n",
        "    :param savepath: æœ¬åœ°ä¿å­˜è·¯å¾„ï¼Œä¾‹å¦‚ \"./Qwen2-7B\"\n",
        "    :return: ä¸‹è½½å‘½ä»¤å­—ç¬¦ä¸²\n",
        "    \"\"\"\n",
        "    return f\"git clone {git_path} {savepath}\"\n",
        "\n",
        "def generate_quantize_command(model_path, savepath):\n",
        "    from awq import AutoAWQForCausalLM\n",
        "    from transformers import AutoTokenizer\n",
        "\n",
        "    model_path = model_path\n",
        "    quant_path = savepath\n",
        "    quant_config = { \"zero_point\": True, \"q_group_size\": 128, \"w_bit\": 4, \"version\": \"GEMM\" }\n",
        "\n",
        "    # Load model\n",
        "    model = AutoAWQForCausalLM.from_pretrained(\n",
        "        model_path, **{\"low_cpu_mem_usage\": True, \"use_cache\": False}\n",
        "    )\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)\n",
        "\n",
        "    # Quantize\n",
        "    model.quantize(tokenizer, quant_config=quant_config)\n",
        "\n",
        "    # Save quantized model\n",
        "    model.save_quantized(quant_path)\n",
        "    tokenizer.save_pretrained(quant_path)\n",
        "\n",
        "    return(f'Model is quantized and saved at \"{quant_path}\"')\n",
        "\n",
        "# æ£€æŸ¥æœåŠ¡æ˜¯å¦è¿è¡Œ\n",
        "def is_server_running():\n",
        "    try:\n",
        "        res = requests.get(\"http://localhost:30000/health\")\n",
        "        return res.status_code == 200\n",
        "    except:\n",
        "        return False\n",
        "\n",
        "# è·å–å½“å‰æœåŠ¡çŠ¶æ€\n",
        "def get_status():\n",
        "    return \"ğŸŸ¢ è¿è¡Œä¸­\" if is_server_running() else \"ğŸ”´ æœªè¿è¡Œ\"\n",
        "\n",
        "def model_predict(message, chat_history):\n",
        "    try:\n",
        "        response = requests.post(\"http://localhost:30000/generate\", json={\n",
        "            \"text\": message,\n",
        "            \"sampling_params\": {\n",
        "                \"temperature\": 0.7,\n",
        "                \"max_new_tokens\": 256\n",
        "            }\n",
        "        })\n",
        "        response.raise_for_status()\n",
        "        data = response.json()\n",
        "\n",
        "        chat_history.append({\"role\": \"user\", \"content\": message})\n",
        "        chat_history.append({\"role\": \"assistant\", \"content\": data[\"text\"]})\n",
        "\n",
        "        return \"\", chat_history\n",
        "\n",
        "    except requests.exceptions.ConnectionError:\n",
        "        return \"\", [{\"role\": \"system\", \"content\": \"âŒ æ— æ³•è¿æ¥åˆ°æœåŠ¡ï¼Œè¯·å…ˆå¯åŠ¨æ¨¡å‹æœåŠ¡å™¨\"}]\n",
        "    except Exception as e:\n",
        "        return \"\", [{\"role\": \"system\", \"content\": f\"âŒ é”™è¯¯: {str(e)}\"}]"
      ],
      "metadata": {
        "id": "zhZjlHr3hwQ6"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# åˆ›å»º Gradio ç•Œé¢\n",
        "with gr.Blocks(title=\"æç®€å¤§æ¨¡å‹éƒ¨ç½²è„šæœ¬\") as demo:\n",
        "    with gr.Tab(\"ğŸ—ï¸ éƒ¨ç½²æ“ä½œ\"):\n",
        "        gr.Markdown(\"# ğŸ“‹ å‘½ä»¤è¡Œæ‰§è¡Œå™¨\\nåœ¨ä¸‹æ–¹è¾“å…¥å‘½ä»¤ï¼ŒæŒ‰å›è½¦æ‰§è¡Œ\")\n",
        "\n",
        "        model_path_input = gr.Textbox(\n",
        "            label=\"æ¨¡å‹è·¯å¾„\",\n",
        "            placeholder=\"ä¾‹å¦‚ï¼šQwen/Qwen3-0.6B\",\n",
        "            value=\"Qwen/Qwen3-0.6B\"\n",
        "        )\n",
        "\n",
        "        with gr.Row():\n",
        "            deploy_type = gr.Radio(\n",
        "                choices=[\"sglang\", \"vllm serve\"],\n",
        "                label=\"éƒ¨ç½²ç±»å‹\",\n",
        "                value=\"sglang\",\n",
        "                scale=3\n",
        "            )\n",
        "            deploy_button = gr.Button(\"ç¡®è®¤éƒ¨ç½²\", scale=1)\n",
        "\n",
        "        command_input = gr.Textbox(\n",
        "            label=\"è¾“å…¥å‘½ä»¤\",\n",
        "            placeholder=\"ä¾‹å¦‚ï¼šping 8.8.8.8 æˆ– ls -l\",\n",
        "            value=\"echo Hello World\"\n",
        "        )\n",
        "\n",
        "        output_box = gr.Textbox(\n",
        "            label=\"æ—¥å¿—è¾“å‡º\",\n",
        "            lines=20,\n",
        "            max_lines=50,\n",
        "            interactive=False\n",
        "        )\n",
        "\n",
        "        with gr.Row():\n",
        "            stop_button = gr.Button(\"ç»ˆæ­¢è¿›ç¨‹\", variant=\"stop\")\n",
        "            # å‘½ä»¤ç”Ÿæˆ\n",
        "        deploy_button.click(\n",
        "            fn=generate_command,\n",
        "            inputs=[model_path_input, deploy_type],\n",
        "            outputs=command_input\n",
        "        )\n",
        "\n",
        "        # å‘½ä»¤æ‰§è¡Œ\n",
        "        command_input.submit(fn=run_command, inputs=command_input, outputs=output_box, scroll_to_output=True)\n",
        "\n",
        "        # ç»ˆæ­¢å‘½ä»¤\n",
        "        stop_button.click(fn=stop_process, inputs=[], outputs=output_box)\n",
        "\n",
        "\n",
        "    with gr.Tab(\"æœåŠ¡ç›‘æ§ä¸èŠå¤©æµ‹è¯•\"):\n",
        "        gr.Markdown(\"# ğŸ¤– å¤§æ¨¡å‹æœåŠ¡ç›‘æ§\")\n",
        "\n",
        "        status_box = gr.Textbox(\n",
        "            label=\"ğŸ“¡ æœåŠ¡çŠ¶æ€\",\n",
        "            value=get_status,\n",
        "            every=3,\n",
        "            interactive=False\n",
        "        )\n",
        "\n",
        "        with gr.Row():\n",
        "            chatbot = gr.Chatbot(\n",
        "                label=\"å¯¹è¯å†å²\",\n",
        "                bubble_full_width=False,\n",
        "                type=\"messages\"\n",
        "            )\n",
        "            msg = gr.Textbox(\n",
        "                label=\"è¾“å…¥é—®é¢˜\",\n",
        "                lines=5,\n",
        "                placeholder=\"è¾“å…¥ä½ çš„é—®é¢˜...\",\n",
        "            )\n",
        "\n",
        "        with gr.Row():\n",
        "            clear = gr.ClearButton([msg, chatbot])\n",
        "            send_button = gr.Button(\"å‘é€\", variant=\"primary\")\n",
        "\n",
        "        msg.submit(fn=model_predict, inputs=[msg, chatbot], outputs=[msg, chatbot], queue=True)\n",
        "        send_button.click(fn=model_predict, inputs=[msg, chatbot], outputs=[msg, chatbot])\n",
        "\n",
        "    with gr.Tab(\"æ¨¡å‹ä¸‹è½½\"):\n",
        "        gr.Markdown(\"# â¬‡ï¸ æ¨¡å‹ä¸‹è½½\")\n",
        "        with gr.Row():\n",
        "            model_path_input = gr.Textbox(\n",
        "                label=\"æ¨¡å‹è·¯å¾„\",\n",
        "                placeholder=\"https://huggingface.co/Qwen/Qwen3-0.6B\",\n",
        "                value=\"https://huggingface.co/Qwen/Qwen3-0.6B\"\n",
        "            )\n",
        "            savepath_input = gr.Textbox(\n",
        "                label=\"ä¿å­˜è·¯å¾„\",\n",
        "                placeholder=\"./models\",\n",
        "                value=\"./models\"\n",
        "            )\n",
        "            dowload_button = gr.Button(\"ç¡®è®¤ä¸‹è½½\", scale=1)\n",
        "        command_input = gr.Textbox(\n",
        "            label=\"è¾“å…¥å‘½ä»¤\",\n",
        "            placeholder=\"ä¾‹å¦‚ï¼šping 8.8.8.8 æˆ– ls -l\",\n",
        "            value=\"echo Hello World\"\n",
        "        )\n",
        "\n",
        "        output_box = gr.Textbox(\n",
        "            label=\"æ—¥å¿—è¾“å‡º\",\n",
        "            lines=20,\n",
        "            max_lines=50,\n",
        "            interactive=False\n",
        "        )\n",
        "\n",
        "\n",
        "        with gr.Row():\n",
        "            stop_button = gr.Button(\"ç»ˆæ­¢è¿›ç¨‹\", variant=\"stop\")\n",
        "            # å‘½ä»¤ç”Ÿæˆ\n",
        "        dowload_button.click(\n",
        "            fn=generate_download_command,\n",
        "            inputs=[model_path_input, savepath_input],\n",
        "            outputs=command_input\n",
        "        )\n",
        "        # å‘½ä»¤æ‰§è¡Œ\n",
        "        command_input.submit(fn=run_command, inputs=command_input, outputs=output_box, scroll_to_output=True)\n",
        "\n",
        "        # ç»ˆæ­¢å‘½ä»¤\n",
        "        stop_button.click(fn=stop_process, inputs=[], outputs=output_box)\n",
        "    with gr.Tab(\"æ¨¡å‹é‡åŒ–\"):\n",
        "        gr.Markdown(\"# ğŸ”¹æ¨¡å‹é‡åŒ–\")\n",
        "        with gr.Row():\n",
        "            model_path_input = gr.Textbox(\n",
        "                label=\"æ¨¡å‹è·¯å¾„\",\n",
        "                placeholder=\"https://huggingface.co/Qwen/Qwen3-0.6B\",\n",
        "                value=\"https://huggingface.co/Qwen/Qwen3-0.6B\"\n",
        "            )\n",
        "            savepath_input = gr.Textbox(\n",
        "                label=\"ä¿å­˜è·¯å¾„\",\n",
        "                placeholder=\"./models\",\n",
        "                value=\"./models\"\n",
        "            )\n",
        "            dowload_button = gr.Button(\"ç¡®è®¤é‡åŒ–\", scale=1)\n",
        "        command_input = gr.Textbox(\n",
        "            label=\"é‡åŒ–ç»“æœ\",\n",
        "        )\n",
        "\n",
        "        output_box = gr.Textbox(\n",
        "            label=\"æ—¥å¿—è¾“å‡º\",\n",
        "            lines=20,\n",
        "            max_lines=50,\n",
        "            interactive=False\n",
        "        )\n",
        "\n",
        "\n",
        "        with gr.Row():\n",
        "            stop_button = gr.Button(\"ç»ˆæ­¢è¿›ç¨‹\", variant=\"stop\")\n",
        "            # å‘½ä»¤ç”Ÿæˆ\n",
        "        dowload_button.click(\n",
        "            fn=generate_quantize_command,\n",
        "            inputs=[model_path_input, savepath_input],\n",
        "            outputs=command_input\n",
        "        )\n",
        "        # å‘½ä»¤æ‰§è¡Œ\n",
        "        command_input.submit(fn=run_command, inputs=command_input, outputs=output_box, scroll_to_output=True)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    demo.launch(share=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 625
        },
        "id": "phtiZExgIakx",
        "outputId": "8a844d65-0009-4631-d67b-f3c006981369"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-9-c6be29598f15>:61: DeprecationWarning: The 'bubble_full_width' parameter is deprecated and will be removed in a future version. This parameter no longer has any effect.\n",
            "  chatbot = gr.Chatbot(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://b4e45fe17eaa7a8e67.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://b4e45fe17eaa7a8e67.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}