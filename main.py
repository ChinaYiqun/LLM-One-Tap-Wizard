import gradio as gr
import subprocess
import shlex
import signal
import requests

# ç”¨äºä¿å­˜å½“å‰è¿è¡Œçš„å­è¿›ç¨‹
current_process = None

def run_command(command):
    """æ‰§è¡Œå‘½ä»¤å¹¶å®æ—¶è¿”å›è¾“å‡º"""
    global current_process
    try:
        args = shlex.split(command)
        process = subprocess.Popen(
            args,
            stdout=subprocess.PIPE,
            stderr=subprocess.STDOUT,
            text=True,
            bufsize=1,
            universal_newlines=True
        )
        current_process = process  # ä¿å­˜å½“å‰è¿›ç¨‹å¯¹è±¡

        while True:
            output = process.stdout.readline()
            if output == '' and process.poll() is not None:
                break
            if output:
                yield output.strip()

        remaining = process.stdout.read()
        if remaining:
            yield remaining.strip()

    except Exception as e:
        yield f"é”™è¯¯: {str(e)}"
    finally:
        current_process = None  # æ¸…é™¤å½“å‰è¿›ç¨‹å¼•ç”¨

def stop_process():
    """ç»ˆæ­¢å½“å‰è¿è¡Œçš„å­è¿›ç¨‹"""
    global current_process
    if current_process is not None:
        # å…ˆå°è¯•ä¼˜é›…ç»ˆæ­¢
        current_process.send_signal(signal.SIGTERM)
        try:
            current_process.wait(timeout=0.5)
        except subprocess.TimeoutExpired:
            # è‹¥ä»æœªå…³é—­ï¼Œå¼ºåˆ¶æ€æ­»
            current_process.kill()
        return "è¿›ç¨‹å·²ç»ˆæ­¢"
    else:
        return "å½“å‰æ²¡æœ‰è¿è¡Œçš„è¿›ç¨‹"


#ç”Ÿæˆå¤§æ¨¡å‹éƒ¨ç½²å‘½ä»¤
def generate_command(model_path, deploy_type):
    """æ ¹æ®æ¨¡å‹è·¯å¾„å’Œéƒ¨ç½²ç±»å‹ç”Ÿæˆå®Œæ•´å‘½ä»¤"""
    common_params = "--host 0.0.0.0 --port 30000"
    if deploy_type == "sglang":
        return f"python3 -m sglang.launch_server --model-path {model_path} {common_params}"
    elif deploy_type == "vllm serve":
        return f"vllm serve {model_path} {common_params}"
    else:
        return "é”™è¯¯ï¼šæœªçŸ¥çš„éƒ¨ç½²ç±»å‹"
#ç”Ÿæˆæ¨¡å‹ä¸‹è½½å‘½ä»¤
def generate_download_command(git_path, savepath):
    """
    ç”Ÿæˆæ¨¡å‹ä¸‹è½½å‘½ä»¤
    :param git_path: æ¨¡å‹çš„ Git ä»“åº“åœ°å€ï¼Œä¾‹å¦‚ "https://huggingface.co/Qwen/Qwen2-7B "
    :param savepath: æœ¬åœ°ä¿å­˜è·¯å¾„ï¼Œä¾‹å¦‚ "./Qwen2-7B"
    :return: ä¸‹è½½å‘½ä»¤å­—ç¬¦ä¸²
    """
    return f"git clone {git_path} {savepath}"

def generate_quantize_command(model_path, savepath):
    from awq import AutoAWQForCausalLM
    from transformers import AutoTokenizer

    model_path = model_path
    quant_path = savepath
    quant_config = { "zero_point": True, "q_group_size": 128, "w_bit": 4, "version": "GEMM" }

    # Load model
    model = AutoAWQForCausalLM.from_pretrained(
        model_path, **{"low_cpu_mem_usage": True, "use_cache": False}
    )
    tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)

    # Quantize
    model.quantize(tokenizer, quant_config=quant_config)

    # Save quantized model
    model.save_quantized(quant_path)
    tokenizer.save_pretrained(quant_path)

    return(f'Model is quantized and saved at "{quant_path}"')

# æ£€æŸ¥æœåŠ¡æ˜¯å¦è¿è¡Œ
def is_server_running():
    try:
        res = requests.get("http://localhost:30000/health")
        return res.status_code == 200
    except:
        return False

# è·å–å½“å‰æœåŠ¡çŠ¶æ€
def get_status():
    return "ğŸŸ¢ è¿è¡Œä¸­" if is_server_running() else "ğŸ”´ æœªè¿è¡Œ"

def model_predict(message, chat_history):
    try:
        response = requests.post("http://localhost:30000/generate", json={
            "text": message,
            "sampling_params": {
                "temperature": 0.7,
                "max_new_tokens": 256
            }
        })
        response.raise_for_status()
        data = response.json()

        chat_history.append({"role": "user", "content": message})
        chat_history.append({"role": "assistant", "content": data["text"]})

        return "", chat_history

    except requests.exceptions.ConnectionError:
        return "", [{"role": "system", "content": "âŒ æ— æ³•è¿æ¥åˆ°æœåŠ¡ï¼Œè¯·å…ˆå¯åŠ¨æ¨¡å‹æœåŠ¡å™¨"}]
    except Exception as e:
        return "", [{"role": "system", "content": f"âŒ é”™è¯¯: {str(e)}"}]


# åˆ›å»º Gradio ç•Œé¢
with gr.Blocks(title="æç®€å¤§æ¨¡å‹éƒ¨ç½²è„šæœ¬") as demo:
    with gr.Tab("ğŸ—ï¸ éƒ¨ç½²æ“ä½œ"):
        gr.Markdown("# ğŸ“‹ å‘½ä»¤è¡Œæ‰§è¡Œå™¨\nåœ¨ä¸‹æ–¹è¾“å…¥å‘½ä»¤ï¼ŒæŒ‰å›è½¦æ‰§è¡Œ")

        model_path_input = gr.Textbox(
            label="æ¨¡å‹è·¯å¾„",
            placeholder="ä¾‹å¦‚ï¼šQwen/Qwen3-0.6B",
            value="Qwen/Qwen3-0.6B"
        )

        with gr.Row():
            deploy_type = gr.Radio(
                choices=["sglang", "vllm serve"],
                label="éƒ¨ç½²ç±»å‹",
                value="sglang",
                scale=3
            )
            deploy_button = gr.Button("ç¡®è®¤éƒ¨ç½²", scale=1)

        command_input = gr.Textbox(
            label="è¾“å…¥å‘½ä»¤",
            placeholder="ä¾‹å¦‚ï¼šping 8.8.8.8 æˆ– ls -l",
            value="echo Hello World"
        )

        output_box = gr.Textbox(
            label="æ—¥å¿—è¾“å‡º",
            lines=20,
            max_lines=50,
            interactive=False
        )

        with gr.Row():
            stop_button = gr.Button("ç»ˆæ­¢è¿›ç¨‹", variant="stop")
            # å‘½ä»¤ç”Ÿæˆ
        deploy_button.click(
            fn=generate_command,
            inputs=[model_path_input, deploy_type],
            outputs=command_input
        )

        # å‘½ä»¤æ‰§è¡Œ
        command_input.submit(fn=run_command, inputs=command_input, outputs=output_box, scroll_to_output=True)

        # ç»ˆæ­¢å‘½ä»¤
        stop_button.click(fn=stop_process, inputs=[], outputs=output_box)


    with gr.Tab("æœåŠ¡ç›‘æ§ä¸èŠå¤©æµ‹è¯•"):
        gr.Markdown("# ğŸ¤– å¤§æ¨¡å‹æœåŠ¡ç›‘æ§")

        status_box = gr.Textbox(
            label="ğŸ“¡ æœåŠ¡çŠ¶æ€",
            value=get_status,
            every=3,
            interactive=False
        )

        with gr.Row():
            chatbot = gr.Chatbot(
                label="å¯¹è¯å†å²",
                bubble_full_width=False,
                type="messages"
            )
            msg = gr.Textbox(
                label="è¾“å…¥é—®é¢˜",
                lines=5,
                placeholder="è¾“å…¥ä½ çš„é—®é¢˜...",
            )

        with gr.Row():
            clear = gr.ClearButton([msg, chatbot])
            send_button = gr.Button("å‘é€", variant="primary")

        msg.submit(fn=model_predict, inputs=[msg, chatbot], outputs=[msg, chatbot], queue=True)
        send_button.click(fn=model_predict, inputs=[msg, chatbot], outputs=[msg, chatbot])

    with gr.Tab("æ¨¡å‹ä¸‹è½½"):
        gr.Markdown("# â¬‡ï¸ æ¨¡å‹ä¸‹è½½")
        with gr.Row():
            model_path_input = gr.Textbox(
                label="æ¨¡å‹è·¯å¾„",
                placeholder="https://huggingface.co/Qwen/Qwen3-0.6B",
                value="https://huggingface.co/Qwen/Qwen3-0.6B"
            )
            savepath_input = gr.Textbox(
                label="ä¿å­˜è·¯å¾„",
                placeholder="./models",
                value="./models"
            )
            dowload_button = gr.Button("ç¡®è®¤ä¸‹è½½", scale=1)
        command_input = gr.Textbox(
            label="è¾“å…¥å‘½ä»¤",
            placeholder="ä¾‹å¦‚ï¼šping 8.8.8.8 æˆ– ls -l",
            value="echo Hello World"
        )

        output_box = gr.Textbox(
            label="æ—¥å¿—è¾“å‡º",
            lines=20,
            max_lines=50,
            interactive=False
        )


        with gr.Row():
            stop_button = gr.Button("ç»ˆæ­¢è¿›ç¨‹", variant="stop")
            # å‘½ä»¤ç”Ÿæˆ
        dowload_button.click(
            fn=generate_download_command,
            inputs=[model_path_input, savepath_input],
            outputs=command_input
        )
        # å‘½ä»¤æ‰§è¡Œ
        command_input.submit(fn=run_command, inputs=command_input, outputs=output_box, scroll_to_output=True)

        # ç»ˆæ­¢å‘½ä»¤
        stop_button.click(fn=stop_process, inputs=[], outputs=output_box)
    with gr.Tab("æ¨¡å‹é‡åŒ–"):
        gr.Markdown("# ğŸ”¹æ¨¡å‹é‡åŒ–")
        with gr.Row():
            model_path_input = gr.Textbox(
                label="æ¨¡å‹è·¯å¾„",
                placeholder="https://huggingface.co/Qwen/Qwen3-0.6B",
                value="https://huggingface.co/Qwen/Qwen3-0.6B"
            )
            savepath_input = gr.Textbox(
                label="ä¿å­˜è·¯å¾„",
                placeholder="./models",
                value="./models"
            )
            dowload_button = gr.Button("ç¡®è®¤é‡åŒ–", scale=1)
        command_input = gr.Textbox(
            label="é‡åŒ–ç»“æœ",
        )

        output_box = gr.Textbox(
            label="æ—¥å¿—è¾“å‡º",
            lines=20,
            max_lines=50,
            interactive=False
        )


        with gr.Row():
            stop_button = gr.Button("ç»ˆæ­¢è¿›ç¨‹", variant="stop")
            # å‘½ä»¤ç”Ÿæˆ
        dowload_button.click(
            fn=generate_quantize_command,
            inputs=[model_path_input, savepath_input],
            outputs=command_input
        )
        # å‘½ä»¤æ‰§è¡Œ
        command_input.submit(fn=run_command, inputs=command_input, outputs=output_box, scroll_to_output=True)


if __name__ == "__main__":
    demo.launch(share=True)
